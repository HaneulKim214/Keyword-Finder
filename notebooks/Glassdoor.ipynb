{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pymongo to store all info from LinkedIN\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "db = client.linkedin_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chrome_driver_path = os.path.abspath(r\"C:\\Users\\haneu\\Desktop\\Data Analytics\\6_mongo_webScrape\\chromedriver.exe\")\n",
    "executable_path = {'executable_path': \"chromedriver.exe\"}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "url = \"https://www.glassdoor.ca/index.htm\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What job are you looking for?\n",
      "data analyst\n",
      "Where do you want to find your job?\n",
      "korea\n"
     ]
    }
   ],
   "source": [
    "print(\"What job are you looking for?\")\n",
    "job = input()\n",
    "job_type = browser.find_by_id(\"KeywordSearch\")\n",
    "job_type.fill(job)\n",
    "\n",
    "print(\"Where do you want to find your job?\")\n",
    "job_location = input()\n",
    "location = browser.find_by_id(\"LocationSearch\")\n",
    "location.fill(job_location)\n",
    "\n",
    "# Clicking button\n",
    "browser.find_by_id(\"HeroSearchButton\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store scraped data\n",
    "company = []\n",
    "location = []\n",
    "job_desc = []\n",
    "position = []\n",
    "\n",
    "def scrape_current_page():\n",
    "\n",
    "    # Getting html of first page\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    jobs = soup.find_all(\"li\", class_=\"jl\")\n",
    "\n",
    "    for job in jobs:\n",
    "        # Store all info into a list         \n",
    "#         position.append(job.find(\"div\", class_=\"jobTitle\").a.text)\n",
    "        \n",
    "        # Pop up will show up as soon as we start scraping.\n",
    "        # so wait few seconds\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # print to see if there was a pop-up\n",
    "        exit_button = soup.find(\"div\", class_=\"ModalStyle__xBtn___34qya\")\n",
    "        \n",
    "        # If there was a pop-up click close button.\n",
    "        if exit_button:\n",
    "            browser.find_by_css(\".ModalStyle__xBtn___34qya\").first.click()\n",
    "        \n",
    "        # Find company name\n",
    "        comp = job.find(\"div\", class_=\"jobHeader\").div.text\n",
    "        print(comp)\n",
    "        # Appending each company name into a list\n",
    "        company.append(comp)\n",
    "        \n",
    "        #same for locations\n",
    "        loc = job.find('div', class_=\"empLoc\").span.text\n",
    "        print(loc)\n",
    "        location.append(loc)\n",
    "        \n",
    "        \n",
    "                \n",
    "        # ------------- Scrape Job descriptions within a page -----------\n",
    "        # job description is in another html, therefore retrieve it once again after\n",
    "        # clicking.\n",
    "        browser.click_link_by_href(job.find(\"a\", class_=\"jobLink\")[\"href\"])\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        job_desc.append(soup.find(\"div\", class_=\"desc\").text)\n",
    "\n",
    "        time.sleep(1) # Since splinter scrapes too fast and skips some job description.\n",
    "    return None\n",
    "\n",
    "def scrape_all():\n",
    "    # grab new html, grab page control elements\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Will throw an error if there is no pagining control => one page => goto except statement\n",
    "    try:\n",
    "        result = soup.find(\"div\", class_=\"pagingControls\").ul\n",
    "        pages = result.find_all(\"li\")\n",
    "\n",
    "        for page in pages:\n",
    "            # scrape each page\n",
    "            scrape_current_page()\n",
    "            # run if <a> exists since un-clickable do not have <a> skipping < and pg1\n",
    "            if page.a:\n",
    "                # within <a> tag click except next button         \n",
    "                if not page.find(\"li\", class_=\"Next\"):\n",
    "                    try:\n",
    "                        # Click to goto next page.\n",
    "                        browser.click_link_by_href(page.a['href'])\n",
    "                    except:\n",
    "                        print(\"This is the last page\")\n",
    "                        break\n",
    "    # only one page\n",
    "    except:\n",
    "        print(\"sad\")\n",
    "        scrape_current_page()\n",
    "        \n",
    "\n",
    "    # No need to return since we appened all data into list\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\n",
      "count down\n",
      "exit_buttonNone\n",
      "Hyperconnect\n",
      "Seoul\n",
      "count down\n",
      "exit_buttonNone\n",
      "Oracle\n",
      "Seoul\n",
      "count down\n",
      "exit_buttonNone\n",
      "Bank of America\n",
      "Seoul\n",
      "count down\n",
      "exit_buttonNone\n",
      "Axiologic Solutions\n",
      "Seoul\n",
      "count down\n",
      "exit_buttonNone\n",
      "IQVIA\n",
      "Seoul\n",
      "count down\n",
      "exit_buttonNone\n",
      "Mercedes-Benz Financial Services Korea Ltd.\n",
      "Seoul\n",
      "count down\n",
      "exit_buttonNone\n",
      "3096\n",
      "Seoul\n",
      "count down\n",
      "exit_buttonNone\n",
      "Daimler\n",
      "Seoul\n",
      "count down\n",
      "exit_buttonNone\n",
      "Bank of America\n",
      "Seoul\n"
     ]
    }
   ],
   "source": [
    "scrape_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hyperconnect', 'Oracle', 'Bank of America', 'Axiologic Solutions', 'IQVIA', 'Mercedes-Benz Financial Services Korea Ltd.', '3096', 'Daimler', 'Bank of America']\n",
      "['Seoul', 'Seoul', 'Seoul', 'Seoul', 'Seoul', 'Seoul', 'Seoul', 'Seoul', 'Seoul']\n"
     ]
    }
   ],
   "source": [
    "print(company)\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking duplicate scrape\n",
    "# Total 30 jobs in each page.\n",
    "print(len(job_desc))\n",
    "print(len(set(job_desc)))\n",
    "job_desc\n",
    "# get list of it and turn it back to a list. \n",
    "# job_desc = set(job_desc)\n",
    "# job_desc = list(job_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------- Text classification to classify technical skill words --------------\n",
    "# Some words are connected with / ex:\"sql/database\", so split them.\n",
    "for job in job_desc:\n",
    "    \", \".join(job.split('/'))\n",
    "job_desc = [\", \".join(job.split('/')) for job in job_desc]\n",
    "\n",
    "# each item is a list of tokenized job_descriptions\n",
    "tok = [nltk.word_tokenize(job.lower()) for job in job_desc]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "def stopword_deleter(tokenized_job_desc):\n",
    "    \"\"\" ignore stop words, bullets, etc. And put it into one list \"\"\"\n",
    "    final_word_list = []\n",
    "    for lists in tokenized_job_desc:\n",
    "        for item in lists:\n",
    "    # --------------------------------- Use REGEX to exclude numbers ------------------\n",
    "            if len(item)>2 and (item not in stop) and (item.isalpha()):\n",
    "                # Some words have \\\\ at the end, remove them.           \n",
    "                final_word_list.append(item.replace(\"\\\\\",\"\"))\n",
    "    return final_word_list\n",
    "\n",
    "\n",
    "# ------------ Lematize\n",
    "cleaned_list = stopword_deleter(tok)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_list = [lemmatizer.lemmatize(word,pos=\"v\") for word in cleaned_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------Taking a look at POS tags -(NOT USED)\n",
    "pos_tag = nltk.pos_tag(lemmatized_list)\n",
    "pos_df = pd.DataFrame(pos_tag, columns=[\"Word\", \"POS\"])\n",
    "pos_sum = pos_df.groupby(\"POS\").count()\n",
    "pos_sum.sort_values([\"Word\"], ascending=False)\n",
    "\n",
    "filtered_pos_tag = []\n",
    "for tag in pos_tag:\n",
    "    if tag[1] ==\"NN\" or tag[1] == \"NNS\" or tag[1] ==\"NNP\" or tag[1] == \"NNPS\":\n",
    "        filtered_pos_tag.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find 100 most frequent words from all scraped job_description\n",
    "freq = nltk.FreqDist(lemmatized_list)\n",
    "most_freq_words = freq.most_common(100)\n",
    "most_freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(most_freq_words, columns=(\"Words\", \"Count\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "                          background_color='white',\n",
    "                          stopwords=stop,\n",
    "                          max_words=100,\n",
    "                          max_font_size=50, \n",
    "                          random_state=42\n",
    "                         ).generate(str(most_freq_words))\n",
    "print(wordcloud)\n",
    "fig = plt.figure(1)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_n_words(cleaned_corpus, n=100):\n",
    "    vec = CountVectorizer().fit(cleaned_corpus)\n",
    "    bag_of_words = vec.transform(cleaned_corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "top_words = get_top_n_words(lemmatized_list, n=100)\n",
    "# top_df = pd.DataFrame(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "new_list = []\n",
    "for key,value in top_words:\n",
    "    new_dict[key] = value\n",
    "new_list.append(new_dict)\n",
    "print(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "final.append(new_list)\n",
    "final.append(my_list)\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For 2-grams\n",
    "def get_top_n2_words(corpus, n=None):\n",
    "    # Consider 2-grams grabbing top 2000 most occuring term\n",
    "    vec1 = CountVectorizer(ngram_range=(1,2),\n",
    "            max_features=2000).fit(corpus) \n",
    "    bag_of_words = vec1.transform(corpus) # Create sparce marix.\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    print(sum_words[0])\n",
    "#     print(vec1.vocabulary_.items())\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in #Select 0 because dict_items has all tuples in a first list., (\"job\", 971) tuple is one item, then as idx increase it select next tuple and so on.\n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) #Sort by number. since (\"job\", 93). x[1] = 93. In descending order.\n",
    "    return words_freq[:n]\n",
    "top2_words = get_top_n2_words(lemmatized_list, n=100)\n",
    "top2_df = pd.DataFrame(top2_words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Freq\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "top_words = get_top_n2_words(lemmatized_list, n=30)\n",
    "top_df = pd.DataFrame(top_words)\n",
    "top_df.columns=[\"Word\", \"Freq\"]\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n",
    "g.set_title(\"1,2-gram words\")\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
