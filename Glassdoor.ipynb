{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 6,
>>>>>>> 0300fed112ba0173b1dadcbba499c8e8ccdb61d6
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import os\n",
    "import pymongo\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pymongo to store all info from LinkedIN\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "db = client.linkedin_db"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 7,
>>>>>>> 0300fed112ba0173b1dadcbba499c8e8ccdb61d6
   "metadata": {},
   "outputs": [],
   "source": [
    "# chrome_driver_path = os.path.abspath(r\"C:\\Users\\haneu\\Desktop\\Data Analytics\\6_mongo_webScrape\\chromedriver.exe\")\n",
    "executable_path = {'executable_path': \"chromedriver.exe\"}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "url = \"https://www.glassdoor.ca/index.htm\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 8,
>>>>>>> 0300fed112ba0173b1dadcbba499c8e8ccdb61d6
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What job are you looking for?\n",
      "Data Scientist\n",
      "Where do you want to find your job?\n",
      "Singapore\n"
     ]
    }
   ],
   "source": [
    "print(\"What job are you looking for?\")\n",
    "job = input()\n",
    "job_type = browser.find_by_id(\"KeywordSearch\")\n",
    "job_type.fill(job)\n",
    "\n",
    "print(\"Where do you want to find your job?\")\n",
    "job_location = input()\n",
    "location = browser.find_by_id(\"LocationSearch\")\n",
    "location.fill(job_location)\n",
    "\n",
    "# Clicking button\n",
    "browser.find_by_id(\"HeroSearchButton\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- loop through pages ------------\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "result = soup.find(\"div\", class_=\"pagingControls\").ul\n",
    "pages = result.find_all(\"li\")\n",
    "\n",
    "print(pages)\n",
    "\n",
    "for page in pages:\n",
    "    # run if <a> exists since un-clickable do not have <a> skipping < and pg1\n",
    "    if page.a:\n",
    "        # within <a> tag click except next button         \n",
    "        if not page.find(\"li\", class_=\"Next\"):\n",
    "            try:\n",
    "                browser.click_link_by_href(page.a['href'])\n",
    "\n",
    "                # --------- call scrape data function here ---------\n",
    "                \n",
    "            except:\n",
    "                print(\"This is the last page\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = []\n",
    "exp_level = []\n",
    "company = []\n",
    "employment_type = []\n",
    "location = []\n",
    "job_desc = []\n",
    "\n",
    "# ------------ Scraping data for each page ------------\n",
    "def scrape():\n",
    "    # Getting html of first page\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    jobs = soup.find_all(\"li\", class_=\"jl\")\n",
    "\n",
    "    for job in jobs:\n",
    "            \n",
    "        position.append(job.find(\"div\", class_=\"jobTitle\").a.text)\n",
    "        # ex: Tommy - Singapore\n",
    "        comp_loc = job.find(\"div\", class_=\"empLoc\").div.text\n",
    "        comp, loc = comp_loc.split(\"â€“\")\n",
    "        # print(comp)\n",
    "        company.append(comp.strip())\n",
    "        location.append(loc.strip())\n",
    "        \n",
    "        browser.click_link_by_href(job.find(\"a\", class_=\"jobLink\")[\"href\"])\n",
    "        \n",
    "        # ------------- Scrape Job descriptions within a page -----------\n",
    "        # from current html since if you click job_posting it render new html\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
<<<<<<< HEAD
    "#         print(soup.find(\"div\", class_=\"desc\").text)\n",
    "        job_desc.append(soup.find(\"div\", class_=\"desc\").text)\n",
    "    \n",
    "        time.sleep(3)\n",
=======
    "        job_desc.append(soup.find(\"div\", class_=\"desc\").text)\n",
>>>>>>> 0300fed112ba0173b1dadcbba499c8e8ccdb61d6
    "        \n",
    "        \n",
<<<<<<< HEAD
    "scrape()"
=======
    "scrape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with',\n",
       " 'financial',\n",
       " 'with',\n",
       " 'data',\n",
       " 'data',\n",
       " 'data',\n",
       " 'financial',\n",
       " 'financial',\n",
       " 'data',\n",
       " 'with',\n",
       " 'with',\n",
       " 'with',\n",
       " 'financial',\n",
       " 'data',\n",
       " 'with',\n",
       " 'data',\n",
       " 'with',\n",
       " 'with',\n",
       " 'with',\n",
       " 'financial',\n",
       " 'financial',\n",
       " 'data']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------- Text classification to classify technical skill words --------------\n",
    "\n",
    "text = job_desc[0]\n",
    "text = text.splitlines()\n",
    "# drop all empty lines.\n",
    "text = [x.strip() for x in text if len(x)>1]\n",
    "\n",
    "# put all item in a list into one then tokenize.\n",
    "one_text = \" \".join(text)\n",
    "\n",
    "tokenized_text = nltk.word_tokenize(one_text)\n",
    "\n",
    "# To use this must put each word as item in list.\n",
    "freq = nltk.FreqDist(tokenized_text)\n",
    "\n",
    "meaningful_words = [word for word in tokenized_text if len(word)>3 and freq[word]>5]\n",
    "meaningful_words"
>>>>>>> 0300fed112ba0173b1dadcbba499c8e8ccdb61d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "28\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- in one page, click on each job posting and grab job description -----\n",
    "def scrape_job_desc():\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 0300fed112ba0173b1dadcbba499c8e8ccdb61d6
   "source": [
    "# checking duplicate scrape\n",
    "# Total 30 jobs in each page.\n",
    "print(len(job_desc))\n",
    "print(len(set(job_desc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------- Text classification to classify technical skill words --------------\n",
    "# combine all job description in a page and analyze keywords\n",
    "\n",
    "# each item is a list of tokenized job_description\n",
    "tok = [nltk.word_tokenize(job.lower()) for job in job_desc]\n",
    "\n",
    "# ignore stop words, bullets, etc. And put it into one list\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def clean_token(what_to_clean):\n",
    "    cleaned_tok = []\n",
    "    for lists in what_to_clean:\n",
    "        for item in lists:\n",
    "            if len(item)>2 and (item not in stop):\n",
    "                cleaned_tok.append(item)\n",
    "    return cleaned_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 211),\n",
       " ('experience', 78),\n",
       " ('learning', 70),\n",
       " ('business', 65),\n",
       " ('team', 53),\n",
       " ('science', 51),\n",
       " ('machine', 48),\n",
       " ('work', 46),\n",
       " ('skills', 45),\n",
       " ('analytics', 42),\n",
       " ('algorithms', 39),\n",
       " ('solutions', 38),\n",
       " ('working', 36),\n",
       " ('knowledge', 36),\n",
       " ('ability', 36),\n",
       " ('problems', 31),\n",
       " ('models', 28),\n",
       " ('insights', 28),\n",
       " ('develop', 28),\n",
       " ('engineering', 27),\n",
       " ('analysis', 26),\n",
       " ('python', 26),\n",
       " ('job', 26),\n",
       " ('research', 24),\n",
       " ('strong', 24),\n",
       " ('using', 23),\n",
       " ('programming', 22),\n",
       " ('techniques', 21),\n",
       " ('technical', 21),\n",
       " ('product', 21),\n",
       " ('role', 20),\n",
       " ('years', 20),\n",
       " ('scientist', 19),\n",
       " ('etc', 19),\n",
       " ('one', 19),\n",
       " ('people', 19),\n",
       " ('based', 19),\n",
       " ('understanding', 18),\n",
       " ('statistics', 18),\n",
       " ('requirements', 18),\n",
       " ('statistical', 18),\n",
       " ('deep', 18),\n",
       " ('related', 18),\n",
       " ('including', 18),\n",
       " ('complex', 17),\n",
       " ('development', 17),\n",
       " ('make', 17),\n",
       " ('computer', 17),\n",
       " ('communication', 17),\n",
       " ('good', 17),\n",
       " ('new', 17),\n",
       " ('optimization', 17),\n",
       " ('solve', 17),\n",
       " ('building', 17),\n",
       " ('teams', 16),\n",
       " ('singapore', 16),\n",
       " ('global', 16),\n",
       " ('products', 16),\n",
       " ('apply', 15),\n",
       " ('advanced', 15),\n",
       " ('degree', 15),\n",
       " ('build', 15),\n",
       " ('mining', 15),\n",
       " ('projects', 15),\n",
       " ('help', 15),\n",
       " ('status', 15),\n",
       " ('design', 14),\n",
       " ('support', 14),\n",
       " ('operations', 14),\n",
       " ('languages', 14),\n",
       " ('tools', 14),\n",
       " ('like', 14),\n",
       " ('customers', 14),\n",
       " ('world', 14),\n",
       " ('company', 14),\n",
       " ('technology', 14),\n",
       " ('analytical', 14),\n",
       " ('sql', 13),\n",
       " ('opportunity', 13),\n",
       " ('lead', 13),\n",
       " ('project', 12),\n",
       " ('key', 12),\n",
       " ('responsibilities', 12),\n",
       " ('mathematics', 12),\n",
       " ('communicate', 12),\n",
       " ('results', 12),\n",
       " ('qualifications', 12),\n",
       " ('services', 12),\n",
       " ('understand', 12),\n",
       " ('big', 12),\n",
       " ('applying', 12),\n",
       " ('customer', 12),\n",
       " ('management', 12),\n",
       " ('industry', 12),\n",
       " ('property', 12),\n",
       " ('provide', 11),\n",
       " ('model', 11),\n",
       " ('improve', 11),\n",
       " ('large', 11),\n",
       " ('predictive', 11)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = nltk.FreqDist(clean_token(tok))\n",
    "most_freq_words = freq.most_common(100)\n",
    "\n",
    "\n",
    "most_freq_words\n",
    "# Now Remove non-technical words...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now use sentiment analysis to find out which category each word belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4,5,6,7,8,9]\n",
    "x[1:9:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
