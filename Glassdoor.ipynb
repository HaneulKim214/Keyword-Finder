{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pymongo to store all info from LinkedIN\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "db = client.linkedin_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chrome_driver_path = os.path.abspath(r\"C:\\Users\\haneu\\Desktop\\Data Analytics\\6_mongo_webScrape\\chromedriver.exe\")\n",
    "executable_path = {'executable_path': \"chromedriver.exe\"}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "url = \"https://www.glassdoor.ca/index.htm\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What job are you looking for?\n",
      "Data Scientist\n",
      "Where do you want to find your job?\n",
      "Singapore\n"
     ]
    }
   ],
   "source": [
    "print(\"What job are you looking for?\")\n",
    "job = input()\n",
    "job_type = browser.find_by_id(\"KeywordSearch\")\n",
    "job_type.fill(job)\n",
    "\n",
    "print(\"Where do you want to find your job?\")\n",
    "job_location = input()\n",
    "location = browser.find_by_id(\"LocationSearch\")\n",
    "location.fill(job_location)\n",
    "\n",
    "# Clicking button\n",
    "browser.find_by_id(\"HeroSearchButton\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- loop through pages ------------\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "result = soup.find(\"div\", class_=\"pagingControls\").ul\n",
    "pages = result.find_all(\"li\")\n",
    "\n",
    "print(pages)\n",
    "\n",
    "for page in pages:\n",
    "    # run if <a> exists since un-clickable do not have <a> skipping < and pg1\n",
    "    if page.a:\n",
    "        # within <a> tag click except next button         \n",
    "        if not page.find(\"li\", class_=\"Next\"):\n",
    "            try:\n",
    "                browser.click_link_by_href(page.a['href'])\n",
    "\n",
    "                # --------- call scrape data function here ---------\n",
    "                \n",
    "            except:\n",
    "                print(\"This is the last page\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = []\n",
    "exp_level = []\n",
    "company = []\n",
    "employment_type = []\n",
    "location = []\n",
    "job_desc = []\n",
    "\n",
    "# ------------ Scraping data for each page ------------\n",
    "def scrape():\n",
    "    # Getting html of first page\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    jobs = soup.find_all(\"li\", class_=\"jl\")\n",
    "\n",
    "    for job in jobs:\n",
    "            \n",
    "        # Store all info into a list         \n",
    "        position.append(job.find(\"div\", class_=\"jobTitle\").a.text)\n",
    "        # ex: Tommy - Singapore\n",
    "        comp_loc = job.find(\"div\", class_=\"empLoc\").div.text\n",
    "        comp, loc = comp_loc.split(\"â€“\")\n",
    "        # print(comp)\n",
    "        company.append(comp.strip())\n",
    "        location.append(loc.strip())\n",
    "        \n",
    "        browser.click_link_by_href(job.find(\"a\", class_=\"jobLink\")[\"href\"])\n",
    "        \n",
    "        # ------------- Scrape Job descriptions within a page -----------\n",
    "        # from current html since if you click job_posting it render new html\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "#         print(soup.find(\"div\", class_=\"desc\").text)\n",
    "        job_desc.append(soup.find(\"div\", class_=\"desc\").text)\n",
    "    \n",
    "        # It is because if you are going too fast it skips some jobs desc.         \n",
    "        time.sleep(3)\n",
    "        \n",
    "        \n",
    "scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking duplicate scrape\n",
    "# Total 30 jobs in each page.\n",
    "print(len(job_desc))\n",
    "print(len(set(job_desc)))\n",
    "\n",
    "# get list of it and turn it back to a list. \n",
    "# job_desc = set(job_desc)\n",
    "# job_desc = list(job_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------- Text classification to classify technical skill words --------------\n",
    "# Some words are connected with / ex:\"sql/database\", so split them.\n",
    "for job in job_desc:\n",
    "    \", \".join(job.split('/'))\n",
    "job_desc = [\", \".join(job.split('/')) for job in job_desc]\n",
    "\n",
    "# each item is a list of tokenized job_descriptions\n",
    "tok = [nltk.word_tokenize(job.lower()) for job in job_desc]\n",
    "\n",
    "\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "def stopword_deleter(tokenized_job_desc):\n",
    "    \"\"\" ignore stop words, bullets, etc. And put it into one list \"\"\"\n",
    "    final_word_list = []\n",
    "    for lists in tokenized_job_desc:\n",
    "        for item in lists:\n",
    "            if len(item)>2 and (item not in stop):\n",
    "                # Some words have \\\\ at the end, remove them.           \n",
    "                final_word_list.append(item.replace(\"\\\\\",\"\"))\n",
    "    return final_word_list\n",
    "# ------------ Lematize\n",
    "cleaned_list = stopword_deleter(tok)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_list = [lemmatizer.lemmatize(word,pos=\"v\") for word in cleaned_list]\n",
    "print(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at POS tags\n",
    "pos_tag = nltk.pos_tag(lemmatized_list)\n",
    "pos_df = pd.DataFrame(pos_tag, columns=[\"Word\", \"POS\"])\n",
    "pos_sum = pos_df.groupby(\"POS\").count()\n",
    "pos_sum.sort_values([\"Word\"], ascending=False)\n",
    "pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pos_tag = []\n",
    "\n",
    "for tag in pos_tag:\n",
    "    if tag[1] ==\"NN\" or tag[1] == \"NNS\" or tag[1] ==\"NNP\" or tag[1] == \"NNPS\":\n",
    "        filtered_pos_tag.append(tag)\n",
    "filtered_pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(filtered_pos_tag)\n",
    "most_freq_words = freq.most_common(100)\n",
    "\n",
    "\n",
    "most_freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find 100 most frequent words\n",
    "freq = nltk.FreqDist(lemmatized_list)\n",
    "most_freq_words = freq.most_common(100)\n",
    "\n",
    "\n",
    "most_freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(most_freq_words, columns=(\"Words\", \"Count\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud(\n",
    "                          background_color='white',\n",
    "                          stopwords=stop,\n",
    "                          max_words=100,\n",
    "                          max_font_size=50, \n",
    "                          random_state=42\n",
    "                         ).generate(str(most_freq_words))\n",
    "print(wordcloud)\n",
    "fig = plt.figure(1)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "vect = CountVectorizer(max_df=0.8, stop_words=stop, ngram_range=(1,2))\n",
    "\n",
    "x = vect.fit_transform(lemmatized_list)\n",
    "list(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(cleaned_corpus, n=None):\n",
    "    vec = CountVectorizer().fit(cleaned_corpus)\n",
    "    bag_of_words = vec.transform(cleaned_corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "top_words = get_top_n_words(lemmatized_list, n=20)\n",
    "top_df = pd.DataFrame(top_words)\n",
    "\n",
    "# Visualize\n",
    "top_words = get_top_n_words(lemmatized_list, n=20)\n",
    "top_df = pd.DataFrame(top_words)\n",
    "top_df.columns=[\"Word\", \"Freq\"]\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n",
    "g.set_title(\"1-gram words\")\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2-grams\n",
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
    "            max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "top2_words = get_top_n2_words(lemmatized_list, n=20)\n",
    "top2_df = pd.DataFrame(top2_words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Freq\"]\n",
    "top2_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
